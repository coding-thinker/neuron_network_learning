{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python391jvsc74a57bd02e7ff9bb4f6a1a4c46f456e81ca28336c30502f6964c0ade8a7bc4ce0bfaaa65",
   "display_name": "Python 3.9.1 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "2e7ff9bb4f6a1a4c46f456e81ca28336c30502f6964c0ade8a7bc4ce0bfaaa65"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Loading site-libs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "import tqdm\n",
    "import scipy.io as sio\n",
    "import os"
   ]
  },
  {
   "source": [
    "# Activation function and its derivative"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dsigmoid(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)"
   ]
  },
  {
   "source": [
    "# NN class definition"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self, input_scale, out_put_scale):\n",
    "        self.input_scale = input_scale\n",
    "        self.output_scale = out_put_scale\n",
    "\n",
    "        # Layer nums: hidden layers and output layer\n",
    "        self.layer_nums = 0\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "\n",
    "    def __str__(self):\n",
    "        scales = [i.shape for i in self.weights]\n",
    "        return str(scales)\n",
    "\n",
    "    def set_inout(self, input_scale, out_put_scale):\n",
    "        self.input_scale = input_scale\n",
    "        self.output_scale = out_put_scale\n",
    "    \n",
    "    def set_layers(self, *layer_scales):\n",
    "        prev_dim = self.input_scale\n",
    "        for scale in layer_scales:\n",
    "            # weights change dim from prev one into next one \n",
    "            # by doing right-side matrix multiplication\n",
    "            self.weights.append(np.random.randn(prev_dim, scale))\n",
    "            self.biases.append(np.random.randn(1, scale))\n",
    "            prev_dim = scale\n",
    "        self.weights.append(np.random.randn(prev_dim, self.output_scale))\n",
    "        self.biases.append(np.random.randn(1, self.output_scale))\n",
    "        self.layer_nums = len(layer_scales) + 1\n",
    "    \n",
    "    def set_lr(self, lr):\n",
    "        self.lr = lr\n",
    "\n",
    "    def predict(self, input_data):\n",
    "        # output class index for each sample\n",
    "        data = input_data.copy()\n",
    "        for weight, bias in zip(self.weights, self.biases):\n",
    "            data = sigmoid(data @ weight + bias) > 0.5\n",
    "        return np.argmax(data, axis=1)\n",
    "\n",
    "    def train(self, input_data, output_data):\n",
    "        # input data with dims: (samples_num, features_num), (samples_num, onehot_length)\n",
    "        # output onehot loss: (samples_num, onehot_length)\n",
    "        \"\"\"\n",
    "        a : output(sigmoided if needed) of layers ; layer 0 as input\n",
    "        z : aw + b\n",
    "        \"\"\"\n",
    "        input_data = input_data.copy()\n",
    "        output_data = output_data.copy()\n",
    "        a = [input_data]\n",
    "        z = [0]\n",
    "\n",
    "        for i in range(self.layer_nums):\n",
    "            # forward propaganda\n",
    "            # z[i+1] = a[i] @ w[i] + b[i] \n",
    "            # a[i+1] = sigmoid(z[i+1]) \n",
    "            z.append(a[i] @ self.weights[i] + self.biases[i])\n",
    "            a.append(sigmoid(z[-1]))\n",
    "\n",
    "        # compute loss\n",
    "        loss = ((a[-1] - output_data) ** 2).sum()\n",
    "        \n",
    "        # back propaganda\n",
    "        d = (a[-1] - output_data) # dl_da\n",
    "\n",
    "        for i in range(self.layer_nums - 1, -1, -1):\n",
    "            # dl_dz = dl_da * da_dz\n",
    "            d *= dsigmoid(z[i + 1])\n",
    "\n",
    "            # dw = dl_dz * dz_da\n",
    "            # left multiplication with a.T\n",
    "            dw = a[i].T @ d\n",
    "\n",
    "            # db = dl_dz * 1\n",
    "            db = d.copy()\n",
    "\n",
    "            # dl_da = dl_dz * dz_da\n",
    "            # right multiplication with w.T\n",
    "            d = d @ self.weights[i].T\n",
    "\n",
    "            # gradient descend\n",
    "            self.weights[i] -= self.lr * dw\n",
    "            self.biases[i] -= self.lr * db.mean(0)\n",
    "        return loss"
   ]
  },
  {
   "source": [
    "# Load data from MNIST"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    # return (train, valid, test)\n",
    "    def prepare(item_set):\n",
    "        X, y = item_set\n",
    "        Y = np.eye(10)[y]\n",
    "        # return x(?, 784), onehot_y(?, 10), index_y(?, )\n",
    "        return (X, Y, y)\n",
    "    with open('mnist.pkl', 'rb') as f:\n",
    "        data_set = pickle.load(f, encoding='iso-8859-1')\n",
    "    ret = []\n",
    "    for item in data_set:\n",
    "        ret.append(prepare(item))\n",
    "    return ret"
   ]
  },
  {
   "source": [
    "# Trainning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib auto\n",
    "if __name__ == '__main__':\n",
    "    (train_x, train_ohy, train_y), (valid_x, valid_ohy, valid_y), (test_x, test_ohy, test_y) = prepare_data()\n",
    "    fn = input('load pre-trained?:').strip()\n",
    "    epoch = 2000\n",
    "    best_acc = 0\n",
    "    try:\n",
    "        si, myNet, train_loss, valid_accuracy = pickle.load(open(fn, 'rb'))\n",
    "    except:\n",
    "        myNet =  NN(input_scale=784, out_put_scale=10)\n",
    "        myNet.set_lr(0.0001)\n",
    "        myNet.set_layers(256, 64)\n",
    "        train_loss = []\n",
    "        valid_accuracy = []\n",
    "        si = 0\n",
    "    plt.ion()\n",
    "    for i in tqdm.tqdm(range(si, epoch)):\n",
    "        train_loss.append(myNet.train(train_x, train_ohy))\n",
    "        valid_accuracy.append((myNet.predict(valid_x) == valid_y).sum() / len(valid_y) * 100)\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(list(range(len(train_loss))), train_loss, color='blue')\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(list(range(len(valid_accuracy))), valid_accuracy, color='red')\n",
    "        plt.pause(0.001)\n",
    "        with open('nn.pkl', 'wb') as f:\n",
    "            pickle.dump([i, myNet, train_loss, valid_accuracy], f)\n",
    "        if best_acc < valid_accuracy[-1]:\n",
    "            best_acc = valid_accuracy[-1]\n",
    "            with open('best.pkl', 'wb') as f:\n",
    "                pickle.dump([i, myNet, train_loss, valid_accuracy], f)\n",
    "    plt.ioff()\n",
    "\n",
    "    plt.plot(list(range(len(train_loss))), train_loss, color='blue')\n",
    "    plt.plot(list(range(len(valid_accuracy))), valid_accuracy, color='red')\n",
    "    plt.show()\n",
    "\n",
    "    print('valid:', valid_accuracy[-1])\n",
    "    print('test:', (myNet.predict(test_x) == test_y).sum() / len(test_y) * 100)"
   ]
  }
 ]
}